{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuvem de palavra: Planos de governo dos candidatos à Prefeitura de Fortaleza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook das nuvens de palavras para a produção de reportagem para o jornal *Diário do Nordeste* sobre os termos que mais se repetem nas propostas de governo dos nove candidatos à Prefeitura de Fortaleza em 2024.\n",
    "\n",
    "A análise aponta que saúde, segurança, educação e trabalho são os termos mais recorrentes. Saúde é a palavra mais citada, em 456 ocasiões. Segurança aparece em seguida, com 214. Ao lado, educação, com 189 menções, e cultura, com 130. Trabalho, inovação, meio ambiente, infraestrutura e renda surgem a seguir. Há ainda termos como vida e áreas, que aparecem de forma recorrente com diferentes aplicações, como \"defesa da vida\" e \"qualidade de vida\".\n",
    "\n",
    "Os dados reforçam avaliações de cientistas políticos que pesquisam a conjuntura local. Para o levantamento, os termos vazios de significado para o plano de governo foram desconsiderados, como pronomes, artigos e preposições. Palavras como \"Fortaleza\", \"política\", \"Prefeitura\", \"projeto\" e \"público\" também foram retiradas para restringir o resultado apenas a propostas dos candidatos.\n",
    "\n",
    "\n",
    "A matéria foi publicada no dia 24 de setembro de 2024 e pode ser lida [neste link](https://diariodonordeste.verdesmares.com.br/pontopoder/quais-palavras-mais-aparecem-nos-planos-de-governo-dos-candidatos-a-prefeitura-de-fortaleza-em-2024-1.3550684), no *Diário do Nordeste*.\n",
    "\n",
    "**Coleta e análise de dados:** Igor Cavalcante\n",
    "\n",
    "**Reportagem:** Igor Cavalcante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta análise está dividida em duas etapas. Na primeira, os dados são tratados, já que eles estão disponibilizados em formato .pdf. Portanto, é necessário transformá-los em .txt e limpá-los. \n",
    "\n",
    "Na segunda etapa, a nuvem de palavras será efetivamente produzida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instale e importe as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca para a criação das nuvens de palavras\n",
    "!pip install wordcloud\n",
    "\n",
    "# Biblioteca para manipular os arquivos .pdf\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Manipulação de texto\n",
    "from wordcloud import WordCloud, STOPWORDS # Criação das nuvens de palavras\n",
    "import matplotlib.pyplot as plt # Visualização dos dados\n",
    "from collections import Counter #Contagem de palavras\n",
    "import pandas as pd # Manipulação e análise dos dados\n",
    "import os # Acesso aos arquivos\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1: Preparação dos arquivos para análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os arquivos cadastrados pelos candidatos no sistema do Tribunal Superior Eleitoral (TSE) ficam registrados na página de cada concorrente ao Executivo a partir [deste link](https://divulgacandcontas.tse.jus.br/divulga/#/home). \n",
    "\n",
    "Os dados são cadastrados em .pdf, portanto, é preciso converter cada um em arquivos .txt.\n",
    "\n",
    "Em seguida, é preciso criar um arquivo reunindo todas as propostas.\n",
    "\n",
    "Obs.: Os candidatos Técio Nunes e André Fernandes apresentaram, inicialmente, uma prévia do plano de governo e, só depois, o documento definitivo. Portanto, os arquivos dos dois foram atualizados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o diretório onde os .pdfs estão localizados e onde os .txts serão salvos\n",
    "base_dir = os.path.dirname(os.path.abspath('nuvem_de_palavras'))\n",
    "pasta_pdf = os.path.join(base_dir, 'planos_pdf/')\n",
    "pasta_txt = os.path.join(base_dir, 'planos_txt/')\n",
    "\n",
    "# Lista dos arquivos .pdf\n",
    "arquivos_pdf = [\n",
    "    'andre_fernandes_atualizado.pdf', 'capitao_wagner.pdf', 'chico_malta.pdf',\n",
    "    'eduardo_girao.pdf', 'evandro_leitao.pdf', 'george_lima.pdf',\n",
    "    'jose_sarto.pdf', 'tecio_nunes_atualizado.pdf', 'ze_batista.pdf'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão e limpeza dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao converter o .pdf surgem alguns caracteres especiais e quebras de linha. Nos planos de governos há ainda letras maiúsculas e minúsculas que não são relevantes aqui, portanto, essa função de limpeza vai remover as quebras de linha, os caracteres especiais, os múltiplos espaços e deixar todos os textos em caixa baixa\n",
    "def limpeza(text):\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text) # Substitui as quebras de linha por espaço\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove caracteres especiais e pontuações, deixando apenas letras, números e espaços\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove múltiplos espaços\n",
    "    return text.lower().strip() # Deixa o texto em caixa baixa\n",
    "\n",
    "# Aqui, cada arquivo .pdf será acessado, seu texto extraído na íntegra a cada página e, em seguida, tudo será concatenado como um só texto\n",
    "for arquivo_pdf in arquivos_pdf:\n",
    "    pdf_path = os.path.join(pasta_pdf, arquivo_pdf)\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() if page.extract_text() else \"\"\n",
    "\n",
    "# Cada texto será submetido à função de limpeza, removendo o que foi citado anteriormente\n",
    "    text_limpo = limpeza(text)\n",
    "\n",
    "# Depois de limpo, o texto é salvo em um arquivo .txt em uma pasta exlusiva para ele\n",
    "    arquivo_txt = arquivo_pdf.replace('.pdf', '.txt')\n",
    "    txt_path = os.path.join(pasta_txt, arquivo_txt)\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text_limpo)\n",
    "\n",
    "# Com o arquivo de cada candidato gerado, agora é produzido um novo arquivo compilando das propostas de todos. Isso será útil para produzir uma nuvem com os temos mais citados por todos os planos\n",
    "with open(os.path.join(pasta_txt, 'arquivo_final.txt'), 'w', encoding='utf-8') as output_file:\n",
    "    for arquivo_pdf in arquivos_pdf:\n",
    "        arquivo_txt = arquivo_pdf.replace('.pdf', '.txt')\n",
    "        txt_path = os.path.join(pasta_txt, arquivo_txt)\n",
    "        \n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            texto = f.read()\n",
    "            output_file.write(texto + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 2: Nuvem de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, a nuvem de palavras será efetivamente criada. \n",
    "\n",
    "Os textos tratados anteriormente passarão por uma segunda \"limpeza\" para excluir as palavras vazias de sentido. Neste caso, além das stopwords da biblioteca, também há termos específicos, adicionados manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o diretório onde os .txts estão localizados e onde as nuvens de palavras serão salvas\n",
    "diretorio = os.path.join(base_dir, 'planos_txt/')\n",
    "arquivos_gerados = os.path.join(base_dir, 'arquivos_gerados/')\n",
    "\n",
    "# Lista dos arquivos .txts\n",
    "arquivos = [\n",
    "    'andre_fernandes_atualizado.txt', 'capitao_wagner.txt', 'chico_malta.txt',\n",
    "    'eduardo_girao.txt', 'evandro_leitao.txt', 'george_lima.txt',\n",
    "    'jose_sarto.txt', 'tecio_nunes_atualizado.txt', 'ze_batista.txt', 'arquivo_final.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo as palavras vazias de sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função gera uma nuvem de palavras a partir do texto fornecido, excluindo palavras que são irrelevantes para a análise\n",
    "def word_cloud(texto_limpo, palavras_vazias):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(palavras_vazias)\n",
    "    wordcloud = WordCloud(width=800, height=400, max_words=50, background_color='white', stopwords=stopwords).generate(texto_limpo)\n",
    "    return wordcloud\n",
    "\n",
    "# Palavras acrescentadas manualmente que não têm significado relevante nas propostas de governo\n",
    "palavras_vazias = {\"a\", \"à\", \"á\", \"ã\", \"ao\", \"é\", \"ainda\", \"além\", \"alguma\", \"algumas\", \"algum\", \"alguns\", \"ampla\", \"amplas\", \"amplo\", \"amplos\", \"ante\", \"após\", \"até\", \"através\", \"cada\", \"coisa\", \"coisas\", \"com\", \"como\", \"pra\", \"dá\", \"fazer\", \"garantir\", \"contra\", \"contudo\", \"da\", \"das\", \"de\", \"dela\", \"delas\", \"dele\", \"deles\", \"depois\", \"serão\", \"recurso\", \"dessa\", \"dessas\", \"desse\", \"desses\", \"desta\", \"destas\", \"deste\", \"destes\", \"disso\", \"disto\", \"do\", \"dos\", \"durante\", \"e\", \"ela\", \"elas\", \"ele\", \"eles\", \"em\", \"então\", \"entre\", \"era\", \"essa\", \"essas\", \"esse\", \"esses\", \"esta\", \"estas\", \"este\", \"estes\", \"eu\", \"foi\", \"foram\", \"há\", \"isso\", \"isto\", \"já\", \"la\", \"lá\", \"lhe\", \"tenham\", \"lhes\", \"lo\", \"mas\", \"me\", \"mesma\", \"mesmas\", \"mesmo\", \"mesmos\", \"meu\", \"meus\", \"minha\", \"minhas\", \"muita\", \"muitas\", \"muito\", \"muitos\", \"na\", \"nas\", \"nem\", \"no\", \"nos\", \"nós\", \"nossa\", \"nossas\", \"nosso\", \"nossos\", \"num\", \"numa\", \"o\", \"os\", \"ou\", \"onde\", \"para\", \"pela\", \"pelas\", \"pelo\", \"pelos\", \"por\", \"qual\", \"quando\", \"que\", \"assim\", \"quem\", \"se\", \"seja\", \"sem\", \"seu\", \"seus\", \"só\", \"sob\", \"sobre\", \"sua\", \"suas\", \"tal\", \"também\", \"te\", \"tem\", \"tendo\", \"ter\", \"teu\", \"teus\", \"toda\", \"todas\", \"todo\", \"todos\", \"tu\", \"tua\", \"tuas\", \"tudo\", \"um\", \"uma\", \"você\", \"vocês\", \"vos\", \"vosso\", \"vossos\", \"será\", \"Fortaleza\", \"programa\", \"na\", \"proposta\", \"garantindo\", \"cidade\", \"se\", \"governo\", \"população\", \"página\", \"pessoa\", \"mais\", \"política\", \"promover\", \"pela\", \"forma\", \"municipal\", \"plano\", \"gestão\", \"acesso\", \"pessoas\", \"sistema\", \"serviço\", \"não\",\"questões\", \"partir\", \"melhoria\", \"espaço\", \"necessidades\", \"103\", \"aos\", \"às\", \"são\", \"área\", \"eixo\", \"políticas\", \"Prefeitura\", \"Nova\", \"criação\", \"rua\", \"sociedade\", \"público\", \"atualizado\", \"primeiro\", \"aquilo\", \"desde\", \"pública\", \"município\", \"fim\", \"está\", \"s\", \"documento\",\"sendo\", \"estamos\", \"fundamental\", \"continuamente\", \"eleitoral\", \"apenas\", \"vez\", \"hoje\", \"proposições\", \"objetivo\", \"criar\", \"municipais\", \"fortalezense\", \"vamos\", \"equipe\", \"tema\", \"ideias\", \"possam\", \"criando\", \"novas\", \"meio\", \"apenas\", \"deve\", \"feita\", \"sejam\", \"ser\", \"mil\", \"outros\", \"seguirá\", \"desenvolvimento\", \"tratar\", \"inicial\", \"receber\", \"grande\", \"diversas\", \"temos\", \"serviços\", \"atendimento\", \"qualidade\", \"comunidade\", \"social\", \"grande\", \"atendimento\", \"lima\", \"ampliar\", \"implementar\", \"grupo\", \"cidadão\", \"projeto\", \"propostas\", \"fortalecer\", \"participação\", \"construir\", \"direito\", \"povo\", \"processo\", \"eixos\", \"redução\", \"fato\", \"realmente\", \"públicas\", \"públicos\", \"capital\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"20252028\", \"ii\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando a nuvem de palavras de cada candidato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie o loop para gerar nuvens de palavras para cada candidato específico e mais uma considerando todos os postulantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui, cada arquivo será acessado, seu texto será lido, as palavras vazias de sentido ignoradas e a nuvem de palavras será gerada a partir disso\n",
    "for arquivo in arquivos:\n",
    "    caminho_completo = os.path.join(diretorio, arquivo)\n",
    "    with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    texto_limpo = limpeza(text)\n",
    "    nuvem = word_cloud(texto_limpo, palavras_vazias)\n",
    "    \n",
    "# Gerando graficamente a nuvem de palavras\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(nuvem, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Nuvem de Palavras - {arquivo.split('.')[0].replace('_', ' ').title()}\")\n",
    "    plt.show()\n",
    "\n",
    "# Salvando as nuvens de palavras como imagens\n",
    "    nuvem.to_file(os.path.join(arquivos_gerados, f\"nuvem_{arquivo.split('.')[0]}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um arquivo com os termos mais citados que aparecem na nuvem e a quantidade de repetição\n",
    "def gerar_ranking_palavras(arquivos, diretorio, palavras_vazias):\n",
    "    dfs = []\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(palavras_vazias)\n",
    "    for arquivo in arquivos:\n",
    "        caminho_completo = os.path.join(diretorio, arquivo)\n",
    "        with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        texto_limpo = limpeza(texto)\n",
    "        palavras = texto_limpo.split()\n",
    "        palavras_filtradas = [palavra for palavra in palavras if palavra.lower() not in stopwords]\n",
    "        contador = Counter(palavras_filtradas)\n",
    "        df_ranking = pd.DataFrame(contador.most_common(50), columns=[f'Palavra_{arquivo.split(\".\")[0]}', f'Frequência_{arquivo.split(\".\")[0]}'])\n",
    "        dfs.append(df_ranking)\n",
    "\n",
    "    # Concatenando os DataFrames lado a lado\n",
    "    df_final = pd.concat(dfs, axis=1)\n",
    "    return df_final\n",
    "\n",
    "# Gerando o ranking das palavras mais usadas por cada candidato\n",
    "df_ranking_palavras = gerar_ranking_palavras(arquivos, diretorio, palavras_vazias)\n",
    "\n",
    "# Salvando a tabela como .csv\n",
    "df_ranking_palavras.to_csv(os.path.join(arquivos_gerados, 'ranking_palavras_candidatos.csv'), index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
